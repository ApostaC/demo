
# Sharing KV cache across different vLLMs

## Brief introduction

<img width="750" alt="image" src="https://github.com/user-attachments/assets/ea4a07b6-80db-4a22-a769-a3b6c54f7a43">

As shown in the above figure, this demo shows that different vLLM instances can share the prefix KV cache between each other by using LMCache on a single node, so that the KV cache generated by one vLLM instance can be reused by another.

Note that though this demo focuses on single-node case, it can be generalized to allow KV cache sharing between any two vLLM instances in the cluster, as long as they have a commonly-shared NFS disk.

## Setup instructions

### Prerequisites
- 4 Nvidia A6000 or A40 GPU on the same node
- Local SSD disk with peak IO bandwidth > 3GB/s
- `docker compose` installed on the machine
- sudo access to run `docker compose up`
- A huggingface token with access to `mistralai/Mistral-7B-Instruct-v0.2`. 
- Python >3.10, with pip installed.


#### Customizing this demo

Note that this demo runs the model `mistralai/Mistral-7B-Instruct-v0.2` and stores KV cache under directory `/tmp`. You can customize these two entires by editing the file `.env` under `demo4-compare-with-vllm`.

### Run this demo

Run the following bash scripts:
```
git clone https://github.com/LMCache/demo.git
cd demo/demo4-compare-with-vllm
echo "HF_TOKEN=<your HF token>" >> .env
sudo docker compose up -d
timeout 300 bash -c '
    until curl -X POST localhost:8000/v1/completions > /dev/null 2>&1; do
      echo "waiting for server to start..."
      sleep 1
    done' # wait for the docker compose to be ready for receiving requests
streamlit run frontend.py
```
Please replace `<your HF token>` with your huggingface token in the bash script above.

You will see the following output when the frontend application is ready:
```text
  You can now view your Streamlit app in your browser.

  Local URL: http://localhost:8501
  Network URL: http://<Your Public IP>:8501
  External URL: http://<Your Public IP>:8501
```

### Send your requests to different serving engines  
After opening the above link in your browser, you will see a webpage with the following layout (may need to "preheat" for ~30 seconds).

<img width="700" alt="image" src="https://github.com/user-attachments/assets/da6935e9-30f5-45a2-a13e-1115d1f70998">

- You can select different serving engines in the orange area
  - Two engines are running vLLM (0.6.2) + LMCache
  - Two engines are running official vLLM (0.6.2)
- You can ask the LLM questions in the green area
- The right-hand side (blue area) shows the "context" of the query sent to the LLM.

Try sending different requests to different serving engines.


## Expected results

When the first request is sent to the `original vLLM (A)` or `vLLM w/ LMCache (A)`, both engines will have a similar response delay.

<img width="600" alt="image" src="https://github.com/user-attachments/assets/800efc1a-2ab2-47ab-9873-5ba3f56289cc">

However, when the request with the same context is sent to the `vLLM w/ LMCache (B)`, the engine can load the KV cache shared from `vLLM w/ LMCache (A)`, and thus have a much better response delay.

<img width="600" alt="image" src="https://github.com/user-attachments/assets/e0dc808b-0483-4093-8b6e-a63dda7b1806">




## Clean up

Use `Ctrl+C` to terminate the frontend, and then run `sudo docker compose down` to shut down the service.

## Known issues

Currently, LMCache may impact the decoding speed of the vLLM when storing the KV cache to the disk. This is a known issue, and can be solved by better async implementation soon.
